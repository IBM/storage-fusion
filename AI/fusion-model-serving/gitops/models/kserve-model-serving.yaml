apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: granite-llm
  namespace: default-dsc
  labels:
    app: llm-serving
    model: granite
    version: "1.0"
  annotations:
    # Ensure this is created after ConfigMap
    argocd.argoproj.io/sync-wave: "0"
spec:
  predictor:
    serviceAccountName: default
    containers:
    - name: kserve-container
      # Using vLLM which supports Hugging Face models and OpenAI-compatible API
      # Publicly available image, no pull secret needed
      image: vllm/vllm-openai:latest
      imagePullPolicy: Always
      resources:
        limits:
          nvidia.com/gpu: "1"
          memory: "16Gi"
          cpu: "4"
        requests:
          nvidia.com/gpu: "1"
          memory: "16Gi"
          cpu: "4"
      env:
      # Load Granite model from Hugging Face Hub (publicly available)
      - name: MODEL
        value: "ibm-granite/granite-3.1-2b-base"  # Publicly available Granite model
      - name: HF_HOME
        value: "/tmp/hf_cache"  # Use writable directory for Hugging Face cache
      - name: VLLM_CACHE_ROOT
        value: "/tmp/vllm_cache"  # Use writable directory for vLLM cache
      - name: HOME
        value: "/tmp"  # Set home to writable directory
      # vLLM uses OpenAI-compatible API format
      args:
      - --model
      - ibm-granite/granite-3.1-2b-base  # Direct model name (no variable substitution in args)
      - --host
      - "0.0.0.0"
      - --port
      - "8080"
      - --api-key
      - "EMPTY"  # vLLM requires API key, but we can use a dummy value
      - --trust-remote-code  # Required for some models
      - --gpu-memory-utilization
      - "0.75"  # Use 75% of GPU memory instead of default 90% to avoid memory errors

