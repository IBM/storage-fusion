# Domino Data Labs - IBM Fusion backup and restore
Custom backup/restore recipe (Kenneth Salerno <kpsalerno@us.ibm.com>)

The purpose of this recipe is to backup and restore specific prerequisite
cluster-scoped resources, in additional to the namespace-scoped resources,
namely:
  - ClusterRoles
  - ClusterRoleBindings
  - CustomResourceDefinitions
  - IngressClasses
  - PriorityClasses
  - SecurityContextConstraints

Note: we also exclude certain resources to not bloat the backup and more
importantly to not break OpenShift Disaster Recovery (ODR) and VolSync

This recipe will cover all Domino namespaces with an Application definition:
  - domino-system
    - Fusion recipe saved here
  - domino-platform
    - includes the shared and blob PVCs
    - excludes temporary PVCs generated by VolSync to rsync CephFS volumes
  - domino-compute
    - excludes the shared and blob PVCs which are sharing a PV
    - excludes temporary PVCs generated by scheduled jobs and apps

# 1) Fusion Application YAML
```
apiVersion: application.isf.ibm.com/v1alpha1
kind: Application
metadata:
  name: dominolab
  namespace: ibm-spectrum-fusion-ns
spec:
  appType: dominolab
  includedNamespaces: 
    - domino-system
    - domino-platform
    - domino-compute
```
# 2) Fusion Recipe YAML
```
apiVersion: spp-data-protection.isf.ibm.com/v1alpha1
kind: Recipe
metadata:
  name: domino-cluster-recipe
  namespace: domino-system
spec:
  appType: dominolab
  groups:
  - name: platform-volumes
    type: volume
    includedNamespaces:
    - domino-platform
    labelSelector: app.kubernetes.io/created-by notin (volsync)
  - name: compute-volumes
    type: volume
    includedNamespaces:
    - domino-compute
    labelSelector: dominodatalab.com/workload-type notin (Scheduled),dominodatalab.com/workload-type notin (App)
  - name: namespace-resources
    type: resource
    includedNamespaces:
    - domino-system
    - domino-platform
    - domino-compute
    excludedResourceTypes:
    - events
    - events.events.k8s.io
    - imagetags.openshift.io
    - pods
    - subscriptions.operators.coreos.com
    - clusterserviceversions.operators.coreos.com
    - installplans.operators.coreos.com
    - persistentvolumes
    - persistentvolumeclaims
    - replicasets
    - volumereplications.replication.storage.openshift.io
    - volumegroupreplications.replication.storage.openshift.io
    - replicationsources.volsync.backube
    - replicationdestinations.volsync.backube
    - endpointslices.discovery.k8s.io
    - endpoints
    - volumesnapshots.snapshot.storage.k8s.io
    - volumegroupsnapshots.groupsnapshot.storage.k8s.io
  - name: cluster-resources
    type: resource
    includeClusterResources: true
    includedResourceTypes:
    - clusterroles
    - clusterrolebindings
    - customresourcedefinitions.apiextensions.k8s.io
    - ingressclasses.networking.k8s.io
    - priorityclasses
    - securitycontextconstraints.security.openshift.io
  workflows:
  - failOn: any-error
    name: backup
    sequence:
    - group: cluster-resources
    - group: namespace-resources
    - group: compute-volumes
    - group: platform-volumes
  - failOn: any-error
    name: restore
    sequence:
    - group: platform-volumes
    - group: compute-volumes
    - group: cluster-resources
    - group: namespace-resources
```

# 3) Application and Recipe installation steps
Create Fusion application definition that spans multiple namespaces

Note: For applications deployed on a HCP managed cluster, apply on spoke
```
oc apply -f dominolab-cluster-application.yaml
```
Create recipe in domino-system namespace

Note: For applications deployed on a HCP managed cluster, apply on spoke
```
oc apply -f dominolab-cluster-recipe.yaml
```
Exclude domino-compute Shared and Blob PVCs from backup
```
oc label pvc -n domino-compute \
  domino-shared-store-domino-compute \
  domino-blob-store-domino-compute \
  velero.io/exclude-from-backup=true
```
Also exclude domino-compute Shared and Blob PVCs from RegionalDR
```
oc label pvc -n domino-compute \
  domino-shared-store-domino-compute \
  domino-blob-store-domino-compute \
  ramendr.openshift.io/exclude=true
```
From Hub's Fusion GUI, find Application "dominolab", add to backup policies

Then patch dominolab PolicyAssignments to use custom recipe
```
for i in $(oc get policyassignment -n ibm-spectrum-fusion-ns -o name | \
  grep /dominolab); do
  oc -n ibm-spectrum-fusion-ns \
    patch $i \
      --type merge \
      -p '{
        "spec": {
          "recipe": {
            "name": "domino-cluster-recipe",
            "namespace": "'domino-system'",
            "apiVersion": "spp-data-protection.isf.ibm.com/v1alpha1"
          }
        }
      }'
done
```
Note: although newer releases of Fusion automatically find recipes in a
namespace associated with a policyassignment, for Applications that span
multiple namespaces we need to patch the policyassignment manually here

# 4) Tune Fusion backup and restore settings (defaults will cancel your job)
```
#!/bin/sh

case $1 in
	'hub')
		# Tuning Hub Backup Restore:
		# Long-running backup and restore jobs and increase ephemeral
		# size limit:
		# change backupDatamoverTimeout from 20 minutes to 480
		# (8 hours)
		# change restoreDatamoverTimeout from 20 minutes to 1200
		# (20 hours)
		# change datamoverJobpodEphemeralStorageLimit from 2000Mi to
		# 8000Mi or more
		# Long-running jobs:
		# change cancelJobAfter from 3600000 milliseconds to 72000000
		# (increase from 1 hour to 20 hours)
		# Raise velero memory limits from 2Gi to 12Gi and
		# ephemeral-storage from 500Mi to 30Gi
		# Raise backup-location-deployment limit from 1Gi to 4Gi
		oc patch dataprotectionagent dpagent -n ibm-backup-restore \
			--type merge \
			--patch '{
			  "spec": {
			    "transactionManager": {
			      "backupDatamoverTimeout": "480",
			      "restoreDatamoverTimeout": "1200",
			      "datamoverJobpodEphemeralStorageLimit": "8000Mi"
			    }
			  }
			}'
		oc patch configmap guardian-configmap -n ibm-backup-restore \
			--type merge \
			--patch '{
			  "data": {
			    "backupDatamoverTimeout": "480",
			    "restoreDatamoverTimeout": "1200",
			    "datamoverJobpodEphemeralStorageLimit": "8000Mi"
			  }
			}'
		oc patch deployment job-manager -n ibm-backup-restore \
			--patch '{
			  "spec": {
			    "template": {
			      "spec": {
			        "containers": [
			          {
			            "name": "job-manager-container",
			            "env": [
			              {
			                "name": "cancelJobAfter",
			                "value": "72000000"
			              }
			            ]
			          }
			        ]
			      }
			    }
			  }
			}'
		oc patch dataprotectionapplication velero \
			-n ibm-backup-restore \
			--type merge \
			--patch '{
			  "spec": {
			    "configuration": {
			      "velero": {
			        "podConfig": {
			          "resourceAllocations": {
			            "limits": {
			              "ephemeral-storage": "30Gi",
			              "memory": "12Gi"
			            }
			          }
			        }
			      }
			    }
			  }
		 	}'
		oc scale deployment backup-location-deployment \
			-n ibm-backup-restore --replicas=0
		oc patch deployment backup-location-deployment \
			-n ibm-backup-restore \
			--patch '{
			  "spec": {
			    "template": {
			      "spec": {
			        "containers": [
			          {
			            "name": "backup-location-container",
			            "resources": {
			              "limits": {
			                "memory": "4Gi"
			              }
			            }
			          }
			        ]
			      }
			    }
			  }
			}'
		oc scale deployment backup-location-deployment \
			-n ibm-backup-restore --replicas=1
		;;
	'checkhub')
		echo "dpagent settings:"
		oc get dataprotectionagent dpagent -n ibm-backup-restore \
			-o yaml | grep -e backupDatamoverTimeout \
				-e restoreDatamoverTimeout \
				-e datamoverJobpodEphemeralStorageLimit
		echo "guardian-configmap settings:"
		oc get configmap guardian-configmap -n ibm-backup-restore \
			-o yaml | grep -e backupDatamoverTimeout \
				-e restoreDatamoverTimeout \
				-e datamoverJobpodEphemeralStorageLimit
		echo "job-manager deployment settings:"
		oc get deployment job-manager -n ibm-backup-restore \
			-o yaml | grep -A1 cancelJobAfter
		echo "velero settings:"
		oc get dataprotectionapplication velero -n ibm-backup-restore \
			-o yaml | grep -A3 limits | tail -4
		echo "backup-location-deployment settings:"
		oc get deployment backup-location-deployment \
			-n ibm-backup-restore -o yaml | grep -A3 limits
		;;
	'spoke')
		# Long-running backup and restore jobs and increase ephemeral
		# size limit:
		# change backupDatamoverTimeout from 20 minutes to 480
		# (8 hours)
		# change restoreDatamoverTimeout from 20 minutes to 1200
		# (20 hours)
		# change datamoverJobpodEphemeralStorageLimit from 2000Mi to
		# 8000Mi or more
		# Raise velero memory limits from 2Gi to 12Gi and
		# ephemeral-storage from 500Mi to 30Gi
		oc patch dataprotectionagent \
			ibm-backup-restore-agent-service-instance \
			-n ibm-backup-restore \
			--type merge \
			--patch '{
			  "spec": {
			    "transactionManager": {
			      "backupDatamoverTimeout": "480",
			      "restoreDatamoverTimeout": "1200",
			      "datamoverJobpodEphemeralStorageLimit": "8000Mi"
			    }
			  }
			}'
		oc patch configmap guardian-configmap -n ibm-backup-restore \
			--type merge \
			--patch '{
			  "data": {
			    "backupDatamoverTimeout": "480",
			    "restoreDatamoverTimeout": "1200",
			    "datamoverJobpodEphemeralStorageLimit": "8000Mi"
			  }
			}'
		oc patch dataprotectionapplication velero \
			-n ibm-backup-restore \
			--type merge \
			--patch '{
			  "spec": {
			    "configuration": {
			      "velero": {
			        "podConfig": {
			          "resourceAllocations": {
			            "limits": {
			              "ephemeral-storage": "30Gi",
			              "memory": "12Gi"
			            }
			          }
			        }
			      }
			    }
			  }
			}'
		;;
	'checkspoke')
		echo "dpagent settings:"
		oc get dataprotectionagent \
			ibm-backup-restore-agent-service-instance \
			-n ibm-backup-restore -o yaml | \
			grep -e backupDatamoverTimeout \
				-e restoreDatamoverTimeout \
				-e datamoverJobpodEphemeralStorageLimit
		echo "guardian-configmap settings:"
		oc get configmap guardian-configmap -n ibm-backup-restore \
			-o yaml | grep -e backupDatamoverTimeout \
				-e restoreDatamoverTimeout \
				-e datamoverJobpodEphemeralStorageLimit
		echo "velero settings:"
		oc get dataprotectionapplication velero -n ibm-backup-restore \
			-o yaml | grep -A3 limits | tail -4
		;;
	*) echo "$(basename "$0") [hub|checkhub|spoke|checkspoke]"
esac
```

# 5) Restore steps for Domino Data Labs on a fresh (blank) cluster
   Perform these steps in this explicit order:
   1) Tune Fusion backup and restore settings (see above step 4)
   2) From Fusion GUI: restore Fusion catalog (Fusion service restore) - this
      is required when you have a new blank cluster freshly installed or when
      you lose your catalog
   3) From Fusion GUI: restore application "dominolab"
   4) Note that PVCs domino-shared-store-domino-compute and
      domino-blob-store-domino-compute in the domino-compute namespace were
      intentionally excluded from the backup.

      (The Blob and Shared PVCs in domino-compute must point to the same CephFS
      volumeHandles that the respective PVCs in the "domino-platform" project
      is using, otherwise you will have two independent versions of the blob
      and shared file systems in these two separate projects that will cause
      file-not-found issues in your workspaces when you upload new documents in
      the UI)

      First copy the PV that the domino-shared-store in domino-platform is
      pointing to, give it a unique name (append -copy to its name), and
      remove the uid, creationTimestamp, resourceVersion, finalizers, and
      remove the status and claimRef blocks.

      Then copy the PVC from domino-platform and change the PVC name to
      domino-shared-store-domino-compute, change the PV name to what you
      assigned your PV above, namespace to domino-compute, and remove the uid,
      resourceVersion, creationTimestamp, finalizers, bind/bound annotations
      and remove the status block.

      Repeat this process for the domino-blob-store-domino-compute PVC.

      Here is an automated script to accomplish the above steps:
      ```
      #!/bin/sh
      for i in domino-shared-store domino-blob-store; do
        oc get pv $(oc get pvc $i -n domino-platform \
          -o jsonpath='{.spec.volumeName}') -o json | \
          jq '.metadata.name += "-copy" | del(.metadata.uid) | del(.metadata.creationTimestamp) | del(.metadata.resourceVersion) | del(.metadata.finalizers) | del(.spec.claimRef) | del(.status)' | \
          oc apply -f -
        sleep 2
        oc get pvc $i -n domino-platform -o json | \
          jq '.metadata.name += "-domino-compute" | .metadata.namespace = "domino-compute" | del(.metadata.uid) | del(.metadata.creationTimestamp) | del(.metadata.resourceVersion) | del(.metadata.annotations."pv.kubernetes.io/bind-completed") | del(.metadata.annotations."pv.kubernetes.io/bound-by-controller") | del(.metadata.finalizers) | .spec.volumeName += "-copy" |  del(.status)' | \
          oc apply -f -
      done
      oc label pvc -n domino-compute \
        domino-shared-store-domino-compute \
        domino-blob-store-domino-compute \
        velero.io/exclude-from-backup=true
      oc label pvc -n domino-compute \
        domino-shared-store-domino-compute \
        domino-blob-store-domino-compute \
        ramendr.openshift.io/exclude=true
      ```

   5) From OpenShift Console: label nodes accordingly for your environment to
      schedule compute and platform pods:

      If using HCP NodePools:

      For platform workers:
      ```
      spec:
        nodeLabels:
          dominodatalab.com/node-pool: "platform"
      ```
      For compute workers:
      ```
      spec:
        nodeLabels:
          dominodatalab.com/node-pool: "default"
      ```

   6) Scale deployments down/up that rely on HashiCorp Vault running (their secrets won't be injected until Vault is up)

   Versions < 6.0:
   Validate if restore has generated valid certificates, if not run these
   additional steps:

   7) From OpenShift Console: delete CertificateRequests for hephaestus\*-tls
      in domino-compute NS

   8) From Linux shell: restart Domino using restart script here (this will
      also delete your hephaestus TLS secrets which is why we had to clear
      previous reqs)
      https://support.domino.ai/support/s/article/Restart-Script

# Backup recipe execution log example:
```
2026-02-09 14:45:38 [INFO]: === Backup & recipe validation ===

2026-02-09 14:45:38 [INFO]: App namespace: ibm-spectrum-fusion-ns name: dominolab

2026-02-09 14:45:39 [INFO]: Recipe found with labels = dp.isf.ibm.com/parent-recipe:domino-cluster-recipe and dp.isf.ibm.com/parent-recipe-namespace:domino-system is []

2026-02-09 14:45:39 [INFO]: Effective namespaces of application: ['domino-system', 'domino-platform', 'domino-compute']

2026-02-09 14:45:40 [INFO]: Job: 3609216c-6eeb-4b67-a0fc-a533ccc8d2d8 
Recipe name: domino-cluster-recipe 
Details: name: domino-cluster-recipe, namespace: domino-system, app_type: dominolab, version: 10.1.12, clusterId: 338abb94-c70d-4534-8d3d-080e94d8effb, applicationId: 0969a1ca-4032-4e07-b4e0-d77618fdbeb2, jobId: 3609216c-6eeb-4b67-a0fc-a533ccc8d2d8, namespace_mappings: {}, volume_groups[0]: platform-volumes, volume_groups[1]: compute-volumes, resource_groups[0]: namespace-resources, resource_groups[1]: cluster-resources, workflows[0]: backup, workflows[1]: restore

2026-02-09 14:45:40 [INFO]: Evaluating volume group platform-volumes with 7 pvc candidates...

2026-02-09 14:45:40 [INFO]: Looking for pvcs with label 'app.kubernetes.io/created-by notin (volsync)'

2026-02-09 14:45:40 [INFO]: Looking for pvcs with or_select_labels 'None'

2026-02-09 14:45:44 [INFO]: Evaluating volume group compute-volumes with 7 pvc candidates...

2026-02-09 14:45:44 [INFO]: Looking for pvcs with label 'dominodatalab.com/workload-type notin (Scheduled),dominodatalab.com/workload-type notin (App)'

2026-02-09 14:45:44 [INFO]: Looking for pvcs with or_select_labels 'None'

2026-02-09 14:45:45 [INFO]: The recipe "domino-cluster-recipe" for apptype "dominolab" in namespace "domino-system" was validated.

2026-02-09 14:45:46 [INFO]: === Recipe execution ===

2026-02-09 14:45:46 [INFO]: Job 3609216c-6eeb-4b67-a0fc-a533ccc8d2d8 recipe domino-cluster-recipe starting execution

2026-02-09 14:45:46 [INFO]: Starting workflow "backup" of recipe "domino-system:domino-cluster-recipe" ...

2026-02-09 14:45:46 [INFO]: Executing workflow: backup in context backup

2026-02-09 14:45:46 [INFO]: Start execution sequence "group/cluster-resources" ...

2026-02-09 14:46:15 [INFO]: The backup operation of resources from namespace ['domino-system', 'domino-platform', 'domino-compute'] completed successfully.

2026-02-09 14:46:16 [INFO]: End execution sequence "group/cluster-resources" completed successfully.

2026-02-09 14:46:17 [INFO]: Start execution sequence "group/namespace-resources" ...

2026-02-09 14:46:45 [INFO]: The backup operation of resources from namespace ['domino-system', 'domino-platform', 'domino-compute'] completed successfully.

2026-02-09 14:46:45 [INFO]: End execution sequence "group/namespace-resources" completed successfully.

2026-02-09 14:46:45 [INFO]: Start execution sequence "group/compute-volumes" ...

2026-02-09 14:46:45 [INFO]: Reevaluating inventory...

2026-02-09 14:46:46 [INFO]: Effective namespaces of application: ['domino-system', 'domino-platform', 'domino-compute']

2026-02-09 14:46:46 [INFO]: Job: 3609216c-6eeb-4b67-a0fc-a533ccc8d2d8 
Recipe name: domino-cluster-recipe 
Details: name: domino-cluster-recipe, namespace: domino-system, app_type: dominolab, version: 10.1.12, clusterId: 338abb94-c70d-4534-8d3d-080e94d8effb, applicationId: 0969a1ca-4032-4e07-b4e0-d77618fdbeb2, jobId: 3609216c-6eeb-4b67-a0fc-a533ccc8d2d8, namespace_mappings: {}, volume_groups[0]: platform-volumes, volume_groups[1]: compute-volumes, resource_groups[0]: namespace-resources, resource_groups[1]: cluster-resources, workflows[0]: backup, workflows[1]: restore

2026-02-09 14:46:47 [INFO]: Evaluating volume group platform-volumes with 5 pvc candidates...

2026-02-09 14:46:47 [INFO]: Looking for pvcs with label 'app.kubernetes.io/created-by notin (volsync)'

2026-02-09 14:46:47 [INFO]: Looking for pvcs with or_select_labels 'None'

2026-02-09 14:46:49 [INFO]: Evaluating volume group compute-volumes with 5 pvc candidates...

2026-02-09 14:46:49 [INFO]: Looking for pvcs with label 'dominodatalab.com/workload-type notin (Scheduled),dominodatalab.com/workload-type notin (App)'

2026-02-09 14:46:49 [INFO]: Looking for pvcs with or_select_labels 'None'

2026-02-09 14:46:50 [INFO]: The recipe "domino-cluster-recipe" for apptype "dominolab" in namespace "domino-system" was validated.

2026-02-09 14:46:51 [INFO]: Reevaluation complete: 2 volume groups

2026-02-09 14:46:51 [INFO]: Vols in vg: 1

2026-02-09 14:46:51 [INFO]: Volume from inventory: domino-compute:test-volume1

2026-02-09 14:46:51 [INFO]: Executing VolumeGroup compute-volumes with workflow type backup...

2026-02-09 14:46:51 [INFO]: Including volume: domino-compute:test-volume1

2026-02-09 14:46:51 [INFO]: Starting CSI snapshot of PVC test-volume1

2026-02-09 14:46:58 [INFO]: End execution sequence "group/compute-volumes" completed successfully.

2026-02-09 14:46:58 [INFO]: Start execution sequence "group/platform-volumes" ...

2026-02-09 14:46:59 [INFO]: Reevaluating inventory...

2026-02-09 14:46:59 [INFO]: Effective namespaces of application: ['domino-system', 'domino-platform', 'domino-compute']

2026-02-09 14:47:00 [INFO]: Job: 3609216c-6eeb-4b67-a0fc-a533ccc8d2d8 
Recipe name: domino-cluster-recipe 
Details: name: domino-cluster-recipe, namespace: domino-system, app_type: dominolab, version: 10.1.12, clusterId: 338abb94-c70d-4534-8d3d-080e94d8effb, applicationId: 0969a1ca-4032-4e07-b4e0-d77618fdbeb2, jobId: 3609216c-6eeb-4b67-a0fc-a533ccc8d2d8, namespace_mappings: {}, volume_groups[0]: platform-volumes, volume_groups[1]: compute-volumes, resource_groups[0]: namespace-resources, resource_groups[1]: cluster-resources, workflows[0]: backup, workflows[1]: restore

2026-02-09 14:47:00 [INFO]: Evaluating volume group platform-volumes with 5 pvc candidates...

2026-02-09 14:47:00 [INFO]: Looking for pvcs with label 'app.kubernetes.io/created-by notin (volsync)'

2026-02-09 14:47:00 [INFO]: Looking for pvcs with or_select_labels 'None'

2026-02-09 14:47:02 [INFO]: Evaluating volume group compute-volumes with 5 pvc candidates...

2026-02-09 14:47:02 [INFO]: Looking for pvcs with label 'dominodatalab.com/workload-type notin (Scheduled),dominodatalab.com/workload-type notin (App)'

2026-02-09 14:47:02 [INFO]: Looking for pvcs with or_select_labels 'None'

2026-02-09 14:47:04 [INFO]: The recipe "domino-cluster-recipe" for apptype "dominolab" in namespace "domino-system" was validated.

2026-02-09 14:47:05 [INFO]: Reevaluation complete: 2 volume groups

2026-02-09 14:47:05 [INFO]: Vols in vg: 2

2026-02-09 14:47:05 [INFO]: Volume from inventory: domino-platform:domino-blob-store

2026-02-09 14:47:05 [INFO]: Volume from inventory: domino-platform:domino-shared-store

2026-02-09 14:47:05 [INFO]: Executing VolumeGroup platform-volumes with workflow type backup...

2026-02-09 14:47:05 [INFO]: Including volume: domino-platform:domino-blob-store

2026-02-09 14:47:05 [INFO]: Including volume: domino-platform:domino-shared-store

2026-02-09 14:47:05 [INFO]: Starting CSI snapshot of PVC domino-blob-store

2026-02-09 14:47:05 [INFO]: Starting CSI snapshot of PVC domino-shared-store

2026-02-09 14:47:05 [INFO]: Reevaluation complete: 2 volume groups

2026-02-09 14:47:05 [INFO]: Vols in vg: 2

2026-02-09 14:47:05 [INFO]: Volume from inventory: domino-platform:domino-blob-store

2026-02-09 14:47:05 [INFO]: Volume from inventory: domino-platform:domino-shared-store

2026-02-09 14:47:05 [INFO]: Executing VolumeGroup platform-volumes with workflow type backup...

2026-02-09 14:47:05 [INFO]: Including volume: domino-platform:domino-blob-store

2026-02-09 14:47:05 [INFO]: Including volume: domino-platform:domino-shared-store

2026-02-09 14:47:05 [INFO]: Starting CSI snapshot of PVC domino-blob-store

2026-02-09 14:47:05 [INFO]: Starting CSI snapshot of PVC domino-shared-store

2026-02-09 14:47:24 [INFO]: End execution sequence "group/platform-volumes" completed successfully.

2026-02-09 14:47:24 [INFO]: Execution of workflow backup completed. Number of failed commands: 0, 0 are essential

2026-02-09 14:47:24 [INFO]: Execution of workflow "backup" of recipe "domino-system:domino-cluster-recipe" completed successfully.

2026-02-09 14:47:24 [INFO]: Recipe executed. Fail Count=0, rollback=False, last failed command: 

2026-02-09 14:47:33 [INFO]: === Post recipe execution ===

2026-02-09 14:48:05 [INFO]: Data upload is in progress. Successfully transferred 0.

2026-02-09 14:48:24 [INFO]: Data upload is in progress. Successfully transferred 14.

2026-02-09 14:48:28 [INFO]: Data upload is in progress. Successfully transferred 21.

2026-02-09 14:48:35 [INFO]: Cleanup the snapshots created during the backup.

2026-02-09 14:48:35 [INFO]: Deleting snapshot 94c81b1c-9f9c-4c8e-b24b-5079877ff00a-1770648411.0234575 in ns domino-compute

2026-02-09 14:48:35 [INFO]: Deleting snapshot 6d2c6123-2f0b-4b41-9ec0-078db3772d42-1770648424.5794256 in ns domino-platform

2026-02-09 14:48:35 [INFO]: Deleting snapshot 037b49fc-cf35-49b2-9198-802988d8d3a6-1770648424.6033206 in ns domino-platform

2026-02-09 14:48:35 [INFO]: Cleanup the data uploads created during the backup.

2026-02-09 14:48:40 [INFO]: Copy backup job completed successfully.
```

# Restore recipe execution log example:
```
2026-02-09 14:59:39 [INFO]: === Restore & recipe validation ===

2026-02-09 15:02:09 [INFO]: Creating namespace: domino-system with labels: {'kubernetes.io/metadata.name': 'domino-system', 'pod-security.kubernetes.io/audit': 'restricted', 'pod-security.kubernetes.io/audit-version': 'latest', 'pod-security.kubernetes.io/warn': 'restricted', 'pod-security.kubernetes.io/warn-version': 'latest'} and annotations: {'kubectl.kubernetes.io/last-applied-configuration': '{"apiVersion":"v1","kind":"Namespace","metadata":{"annotations":{"openshift.io/description":"","openshift.io/display-name":"","openshift.io/requester":"system:admin","openshift.io/sa.scc.mcs":"s0:c27,c19","openshift.io/sa.scc.supplemental-groups":"1000740000/10000","openshift.io/sa.scc.uid-range":"1000740000/10000","security.openshift.io/MinimallySufficientPodSecurityStandard":"restricted"},"creationTimestamp":"2025-12-15T07:12:22Z","labels":{"kubernetes.io/metadata.name":"domino-system","pod-security.kubernetes.io/audit":"restricted","pod-security.kubernetes.io/audit-version":"latest","pod-security.kubernetes.io/warn":"restricted","pod-security.kubernetes.io/warn-version":"latest"},"name":"domino-system","resourceVersion":"926986","uid":"333e59b2-8050-48b6-ac82-6d0fbf7f220d"},"spec":{"finalizers":["kubernetes"]},"status":{"phase":"Active"}}\n', 'openshift.io/description': '', 'openshift.io/display-name': '', 'openshift.io/requester': 'system:admin', 'openshift.io/sa.scc.mcs': 's0:c27,c19', 'openshift.io/sa.scc.supplemental-groups': '1000740000/10000', 'openshift.io/sa.scc.uid-range': '1000740000/10000', 'security.openshift.io/MinimallySufficientPodSecurityStandard': 'restricted'}

2026-02-09 15:02:09 [INFO]: Created namespace: domino-system

2026-02-09 15:02:09 [INFO]: Creating namespace: domino-platform with labels: {'kubernetes.io/metadata.name': 'domino-platform', 'pod-security.kubernetes.io/audit': 'restricted', 'pod-security.kubernetes.io/audit-version': 'latest', 'pod-security.kubernetes.io/warn': 'restricted', 'pod-security.kubernetes.io/warn-version': 'latest'} and annotations: {'kubectl.kubernetes.io/last-applied-configuration': '{"apiVersion":"v1","kind":"Namespace","metadata":{"annotations":{"openshift.io/description":"","openshift.io/display-name":"","openshift.io/requester":"system:admin","openshift.io/sa.scc.mcs":"s0:c27,c24","openshift.io/sa.scc.supplemental-groups":"1000750000/10000","openshift.io/sa.scc.uid-range":"1000750000/10000","security.openshift.io/MinimallySufficientPodSecurityStandard":"restricted"},"creationTimestamp":"2025-12-15T07:12:22Z","labels":{"kubernetes.io/metadata.name":"domino-platform","pod-security.kubernetes.io/audit":"restricted","pod-security.kubernetes.io/audit-version":"latest","pod-security.kubernetes.io/warn":"restricted","pod-security.kubernetes.io/warn-version":"latest"},"name":"domino-platform","resourceVersion":"927006","uid":"1c87566a-34e1-44cf-9f2a-61ec540a52be"},"spec":{"finalizers":["kubernetes"]},"status":{"phase":"Active"}}\n', 'openshift.io/description': '', 'openshift.io/display-name': '', 'openshift.io/requester': 'system:admin', 'openshift.io/sa.scc.mcs': 's0:c27,c24', 'openshift.io/sa.scc.supplemental-groups': '1000750000/10000', 'openshift.io/sa.scc.uid-range': '1000750000/10000', 'security.openshift.io/MinimallySufficientPodSecurityStandard': 'restricted'}

2026-02-09 15:02:10 [INFO]: Created namespace: domino-platform

2026-02-09 15:02:10 [INFO]: Creating namespace: domino-compute with labels: {'kubernetes.io/metadata.name': 'domino-compute', 'pod-security.kubernetes.io/audit': 'restricted', 'pod-security.kubernetes.io/audit-version': 'latest', 'pod-security.kubernetes.io/warn': 'restricted', 'pod-security.kubernetes.io/warn-version': 'latest'} and annotations: {'kubectl.kubernetes.io/last-applied-configuration': '{"apiVersion":"v1","kind":"Namespace","metadata":{"annotations":{"openshift.io/description":"","openshift.io/display-name":"","openshift.io/requester":"system:admin","openshift.io/sa.scc.mcs":"s0:c28,c2","openshift.io/sa.scc.supplemental-groups":"1000760000/10000","openshift.io/sa.scc.uid-range":"1000760000/10000","security.openshift.io/MinimallySufficientPodSecurityStandard":"restricted"},"creationTimestamp":"2025-12-15T07:12:22Z","labels":{"kubernetes.io/metadata.name":"domino-compute","pod-security.kubernetes.io/audit":"restricted","pod-security.kubernetes.io/audit-version":"latest","pod-security.kubernetes.io/warn":"restricted","pod-security.kubernetes.io/warn-version":"latest"},"name":"domino-compute","resourceVersion":"927034","uid":"a2b020dc-715b-48b9-a9a9-90a7ffc03925"},"spec":{"finalizers":["kubernetes"]},"status":{"phase":"Active"}}\n', 'openshift.io/description': '', 'openshift.io/display-name': '', 'openshift.io/requester': 'system:admin', 'openshift.io/sa.scc.mcs': 's0:c28,c2', 'openshift.io/sa.scc.supplemental-groups': '1000760000/10000', 'openshift.io/sa.scc.uid-range': '1000760000/10000', 'security.openshift.io/MinimallySufficientPodSecurityStandard': 'restricted'}

2026-02-09 15:02:10 [INFO]: Created namespace: domino-compute

2026-02-09 15:02:10 [INFO]: Recipe name: domino-cluster-recipe 
Details: name: domino-cluster-recipe, namespace: domino-system, app_type: dominolab, version: 10.1.12, clusterId: daa67777-16e6-481c-9eb7-d78babd429a5, applicationId: 0969a1ca-4032-4e07-b4e0-d77618fdbeb2, jobId: b836892c-08c3-405e-8bd2-1e902c3f0ecb, namespace_mappings: {'domino-system': 'domino-system', 'domino-platform': 'domino-platform', 'domino-compute': 'domino-compute'}, volume_groups[0]: platform-volumes, volume_groups[1]: compute-volumes, resource_groups[0]: namespace-resources, resource_groups[1]: cluster-resources, workflows[0]: backup, workflows[1]: restore

2026-02-09 15:02:10 [WARNING]: Number of potential problems found in recipe "domino-cluster-recipe" for apptype "dominolab" in namespace "domino-system": "2".

2026-02-09 15:02:19 [INFO]: === Recipe execution ===

2026-02-09 15:02:19 [INFO]: Recipe domino-cluster-recipe starting execution

2026-02-09 15:02:19 [INFO]: Starting workflow "restore" of recipe "domino-system:domino-cluster-recipe" ...

2026-02-09 15:02:19 [INFO]: Executing workflow: restore in context restore

2026-02-09 15:02:19 [INFO]: Start execution sequence "group/platform-volumes" ...

2026-02-09 15:02:19 [INFO]: Executing VolumeGroup platform-volumes with workflow type restore...

2026-02-09 15:02:19 [INFO]: Including volume: domino-platform:domino-blob-store

2026-02-09 15:02:19 [INFO]: Including volume: domino-platform:domino-shared-store

2026-02-09 15:03:06 [INFO]: Data download is in progress. Successfully transferred 0.0 B.

2026-02-09 15:03:10 [INFO]: Data download is in progress. Successfully transferred 7.0 B.

2026-02-09 15:03:40 [INFO]: End execution sequence "group/platform-volumes" completed successfully.

2026-02-09 15:03:40 [INFO]: Start execution sequence "group/compute-volumes" ...

2026-02-09 15:03:41 [INFO]: Executing VolumeGroup compute-volumes with workflow type restore...

2026-02-09 15:03:41 [INFO]: Including volume: domino-compute:test-volume1

2026-02-09 15:03:57 [INFO]: Data download is in progress. Successfully transferred 14.0 B.

2026-02-09 15:04:20 [INFO]: End execution sequence "group/compute-volumes" completed successfully.

2026-02-09 15:04:20 [INFO]: Start execution sequence "group/cluster-resources" ...

2026-02-09 15:04:20 [INFO]: The backup operation of resources from namespace ['domino-system', 'domino-platform', 'domino-compute'] completed successfully.

2026-02-09 15:04:20 [INFO]: Executing Velero restore backup-resources-f6013857-fbaa-4ef5-af62-364859d7ecf3 using backup backup-resources-938ae16b-6240-405f-bd98-2c13da2ac146 with include_namespaces ['domino-system', 'domino-platform', 'domino-compute'], exclude_namespaces None, label-selector None, include-resourcetypes ['clusterroles', 'clusterrolebindings', 'customresourcedefinitions.apiextensions.k8s.io', 'ingressclasses.networking.k8s.io', 'priorityclasses', 'securitycontextconstraints.security.openshift.io'], exclude-resourcetypes ['PersistentVolumeClaim', 'imagestreamtags.image.openshift.io', 'virtualmachineinstancemigrations.kubevirt.io', 'virtualmachineclones.clone.kubevirt.io'], include-cluster-resources True, namespace-mapping {}, and restore-overwrite-resource False, or-label-selector None and labelSelectorType None

2026-02-09 15:04:40 [INFO]: End execution sequence "group/cluster-resources" completed successfully.

2026-02-09 15:04:40 [INFO]: Start execution sequence "group/namespace-resources" ...

2026-02-09 15:04:40 [INFO]: The backup operation of resources from namespace ['domino-system', 'domino-compute', 'domino-platform'] completed successfully.

2026-02-09 15:04:40 [INFO]: Executing Velero restore backup-resources-b2e6b7ad-b0bf-4407-af77-927215558b34 using backup backup-resources-8e89d188-d896-413a-9dbc-499e71c70818 with include_namespaces ['domino-system', 'domino-compute', 'domino-platform'], exclude_namespaces None, label-selector None, include-resourcetypes None, exclude-resourcetypes ['events', 'events.events.k8s.io', 'imagetags.openshift.io', 'pods', 'subscriptions.operators.coreos.com', 'clusterserviceversions.operators.coreos.com', 'installplans.operators.coreos.com', 'persistentvolumes', 'persistentvolumeclaims', 'replicasets', 'volumereplications.replication.storage.openshift.io', 'volumegroupreplications.replication.storage.openshift.io', 'replicationsources.volsync.backube', 'replicationdestinations.volsync.backube', 'endpointslices.discovery.k8s.io', 'endpoints', 'volumesnapshots.snapshot.storage.k8s.io', 'volumegroupsnapshots.groupsnapshot.storage.k8s.io', 'PersistentVolumeClaim', 'imagestreamtags.image.openshift.io', 'virtualmachineinstancemigrations.kubevirt.io', 'virtualmachineclones.clone.kubevirt.io'], include-cluster-resources None, namespace-mapping {}, and restore-overwrite-resource False, or-label-selector None and labelSelectorType None

2026-02-09 15:05:00 [INFO]: End execution sequence "group/namespace-resources" completed successfully.

2026-02-09 15:05:00 [INFO]: Execution of workflow restore completed. Number of failed commands: 0, 0 are essential

2026-02-09 15:05:00 [INFO]: Execution of workflow "restore" of recipe "domino-system:domino-cluster-recipe" completed successfully.

2026-02-09 15:05:01 [INFO]: === Post recipe execution ===

2026-02-09 15:05:01 [INFO]: Restore job completed successfully.
```
